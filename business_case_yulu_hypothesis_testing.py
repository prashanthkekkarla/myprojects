# -*- coding: utf-8 -*-
"""Business Case: Yulu - Hypothesis Testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14FS898rDLfJP5Iaqxv5583XNFzjn0sNi

##**About Yulu**

Yulu is India’s leading micro-mobility service provider, which offers unique vehicles for the daily commute. Starting off as a mission to eliminate traffic congestion in India, Yulu provides the safest commute solution through a user-friendly mobile app to enable shared, solo and sustainable commuting.

Yulu zones are located at all the appropriate locations (including metro stations, bus stands, office spaces, residential areas, corporate offices, etc) to make those first and last miles smooth, affordable, and convenient!

Yulu has recently suffered considerable dips in its revenues. They have contracted a consulting company to understand the factors on which the demand for these shared electric cycles depends. Specifically, they want to understand the factors affecting the demand for these shared electric cycles in the Indian market.

##**Business Problem**

The Management team at Walmart Inc. wants to analyze the customer purchase behavior (specifically, purchase amount) against the customer’s gender and the various other factors to help the business make better decisions. They want to understand if the spending habits differ between male and female customers: Do women spend more on Black Friday than men? (Assume 50 million customers are male and 50 million are female).

####Importing the Necessary Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import datetime as dt
import scipy.stats as spy

url = "https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/001/428/original/bike_sharing.csv?1642089089"

yulu = pd.read_csv(url)

"""####**Data Analysis**"""

yulu.shape

yulu.columns

yulu.head()

yulu.tail()

yulu.isnull().sum()

yulu.dtypes

yulu.info()

#Converting the datatype of datetime column from object to datetime
yulu['datetime'] = pd.to_datetime(yulu['datetime'])

"""####What is the duration covered by the data?"""

yulu['datetime'].min()

yulu['datetime'].max()

yulu['datetime'].max() - yulu['datetime'].min()

yulu['day'] = yulu['datetime'].dt.day_name()

# Setting the 'datetime' column as the index of the DataFrame for improved data handling.
yulu.set_index('datetime', inplace=True)

# This indexing approach streamlines data access, making it simpler and more efficient.
# It facilitates tasks like resampling, time-based slicing, and applying time-related calculations.

"""####Slicing the data by time"""

plt.figure(figsize = (16, 8))

# Creating a monthly line plot after resampling the data, and computing the monthly mean values
# for 'casual', 'registered', and 'count' users.
yulu.resample('M')['casual'].mean().plot(kind = 'line', legend = 'casual', marker = 'o')
yulu.resample('M')['registered'].mean().plot(kind = 'line', legend = 'registered', marker = 'o')
yulu.resample('M')['count'].mean().plot(kind = 'line', legend = 'count', marker = 'o')

plt.grid(axis = 'y', linestyle = '--')    # adding gridlines only along the y-axis
plt.yticks(np.arange(0, 301, 20))
plt.ylim(0,)    # setting the lower y-axis limit to 0
plt.show()      # displaying the plot

# This code generates a visualization depicting the monthly total values for the 'casual', 'registered',
# and 'count' variables. It provides a straightforward way to compare and analyze their long-term patterns.

plt.figure(figsize = (16, 8))

# Creating a line plot by resampling the data on a monthly basis and calculating the monthly sum
# of 'casual', 'registered', and 'count' users.
yulu.resample('M')['casual'].sum().plot(kind = 'line', legend = 'casual', marker = 'o')
yulu.resample('M')['registered'].sum().plot(kind = 'line', legend = 'registered', marker = 'o')
yulu.resample('M')['count'].sum().plot(kind = 'line', legend = 'count', marker = 'o')

plt.grid(axis = 'y', linestyle = '--')      # adding gridlines only along the y-axis
plt.yticks(np.arange(0, 130001, 10000))
plt.ylim(0,)        # setting the lower y-axis limit to 0
plt.show()          # displaying the plot

"""####Was there an increase in the average hourly bike rental count from 2011 to 2012?"""

# resampling the DataFrame by the year
yulu1 = yulu.resample('Y')['count'].mean().to_frame().reset_index()

# Create a new column 'prev_count' by shifting the 'count' column one position up
    # to compare the previous year's count with the current year's count
yulu1['prev_count'] = yulu1['count'].shift(1)

# Calculating the growth percentage of 'count' with respect to the 'count' of previous year
yulu1['growth_percent'] = (yulu1['count'] - yulu1['prev_count']) * 100 / yulu1['prev_count']
yulu1

"""* The data shows substantial growth in the variable's count over one year.
* The mean hourly bike rental count was 144 in 2011 and increased to 239 in 2012.
* This represents an annual growth rate of 65.41%.
* These findings indicate positive growth and potentially an increasing demand for the variable being measured.
"""

yulu.reset_index(inplace = True)

"""####What is the variation in the average hourly bike rental count across different months?"""

# Grouping the DataFrame by the month
df1 = yulu.groupby(by = yulu['datetime'].dt.month)['count'].mean().reset_index()
df1.rename(columns = {'datetime' : 'month'}, inplace = True)

# Create a new column 'prev_count' by shifting the 'count' column one position up
    # to compare the previous month's count with the current month's count
df1['prev_count'] = df1['count'].shift(1)

# Calculating the growth percentage of 'count' with respect to the 'count' of previous month
df1['growth_percent'] = (df1['count'] - df1['prev_count']) * 100 / df1['prev_count']
df1.set_index('month', inplace = True)
df1

"""* Rental bike count shows increasing trends from January to March, with a significant 34.70% growth rate between February and March.
* Growth stabilizes from April to June, with smaller increases.
* July to September sees a slight decrease in bike counts, with negative growth rates.
* Counts continue to decline from October to December, with the most significant drop occurring between October and November (-14.94%).
"""

# The created plot displays the monthly average hourly distribution of rental bike counts, providing a means to compare and identify patterns or trends throughout the year.

# Setting the figure size for the plot
plt.figure(figsize = (12, 6))

# Setting the title for the plot
plt.title("The average hourly distribution of count of rental bikes across different months")

# Grouping the DataFrame by the month and calculating the mean of the 'count' column for each month.
    # Ploting the line graph using markers ('o') to represent the average count per month.
yulu.groupby(by = yulu['datetime'].dt.month)['count'].mean().plot(kind = 'line', marker = 'o')

plt.ylim(0,)    # Setting the y-axis limits to start from zero
plt.xticks(np.arange(1, 13))   # Setting the x-ticks to represent the months from 1 to 12
plt.legend('count')    # Adding a legend to the plot for the 'count' line.
plt.yticks(np.arange(0, 400, 50))
# Adding gridlines to both the x and y axes with a dashed line style
plt.grid(axis = 'both', linestyle = '--')
plt.plot()     # Displaing the plot.

"""* Highest average hourly bike counts in June, followed by July and August.
* Lowest average hourly bike counts in January, followed by February and March.
* Suggests a seasonal pattern with higher demand in spring and summer, a slight decline in fall, and further decrease in winter.
* Valuable insights for resource allocation, marketing strategies, and operational planning for the rental bike company throughout the year

####How does the average hourly distribution of rental bike counts vary over the course of a day?
"""

# Convert the 'datetime' column to datetime format
yulu['datetime'] = pd.to_datetime(yulu['datetime'])

# Grouping the DataFrame by the hour and calculating the mean of 'count'
df1 = yulu.groupby(yulu['datetime'].dt.hour)['count'].mean().reset_index()
df1.rename(columns={'datetime': 'hour'}, inplace=True)

# Creating a new column 'prev_count' by shifting the 'count' column up by one position
df1['prev_count'] = df1['count'].shift(1)

# Calculating the growth percentage of 'count' concerning the 'count' of the previous hour
df1['growth_percent'] = ((df1['count'] - df1['prev_count']) * 100 / df1['prev_count']).fillna(0)
df1.set_index('hour', inplace=True)
df1

"""* Early morning hours (hours 0 to 5) exhibit a significant count decrease with negative growth percentages ranging from -38.59% to -48.66%.
* Starting from hour 5, there's a sudden count increase, with a sharp positive growth percentage of 208.52% observed between hours 4 and 5.
* The count continues to rise significantly until it peaks at hour 17, with a growth percentage of 48.17% compared to the previous hour.
* After hour 17, there's a gradual count decrease during the late evening and nighttime hours, with negative growth percentages ranging from -8.08% to -32.99%.
"""

plt.figure(figsize = (12, 6))
plt.title("The distribution of average count of rental bikes on an hourly basis in a single day")
yulu.groupby(by = yulu['datetime'].dt.hour)['count'].mean().plot(kind = 'line', marker = 'o')
plt.ylim(0,)
plt.xticks(np.arange(0, 24))
plt.legend('count')
plt.grid(axis = 'both', linestyle = '--')
plt.plot()

"""* Highest average bike count at 5 PM, followed by 6 PM and 8 AM.
* Lowest average bike count at 4 AM, followed by 3 AM and 5 AM.
* These patterns show distinct fluctuations throughout the day: low counts in the early morning, a sudden increase in the morning, peak counts in the afternoon, and a gradual decline in the evening and nighttime.

####Changing the data type of the season column.
"""

yulu.info()

# 1: spring, 2: summer, 3: fall, 4: winter
def season_category(x):
    if x == 1:
        return 'spring'
    elif x == 2:
        return 'summer'
    elif x == 3:
        return 'fall'
    else:
        return 'winter'
yulu['season'] = yulu['season'].apply(season_category)

print('Memory usage of season column : ', yulu['season'].memory_usage())
# Since the dtype of season column is object, we can convert the dtype to category to save memory
yulu['season'] = yulu['season'].astype('category')
print('Updated Memory usage of season column : ', yulu['season'].memory_usage())

"""####Modifying the data type of the 'workingday' column"""

print('Max value entry in workingday column : ', yulu['workingday'].max())
print('Memory usage of workingday column : ', yulu['workingday'].memory_usage())
# Since the maximum entry in workingday column is 1 and the dtype is int64, we can convert the dtype to category to save memory
yulu['workingday'] = yulu['workingday'].astype('category')
print('Updated Memory usage of workingday column : ', yulu['workingday'].memory_usage())

"""####Modifying the dtype of the weather column"""

print('Max value entry in weather column : ', yulu['weather'].max())
print('Memory usage of weather column : ', yulu['weather'].memory_usage())
# Since the maximum entry in weather column is 4 and the dtype is int64, we can convert the dtype to category to save memory
yulu['weather'] = yulu['weather'].astype('category')
print('Updated Memory usage of weather column : ', yulu['weather'].memory_usage())

"""####Modifying the dtype of the temp column"""

print('Max value entry in temp column : ', yulu['temp'].max())
print('Memory usage of temp column : ', yulu['temp'].memory_usage())
# Since the maximum entry in temp column is 41.0 and the dtype is float64, we can convert the dtype to float32 to save memory
yulu['temp'] = yulu['temp'].astype('float32')
print('Updated Memory usage of temp column : ', yulu['temp'].memory_usage())

"""####Modifying the dtype of the atemp column"""

print('Max value entry in atemp column : ', yulu['atemp'].max())
print('Memory usage of atemp column : ', yulu['atemp'].memory_usage())
# Since the maximum entry in atemp column is 45.455 and the dtype is float64, we can convert the dtype to float32 to save memory
yulu['atemp'] = yulu['atemp'].astype('float32')
print('Updated Memory usage of atemp column : ', yulu['atemp'].memory_usage())

"""####Modifying the dtype of the humidity column"""

print('Max value entry in humidity column : ', yulu['humidity'].max())
print('Memory usage of humidity column : ', yulu['temp'].memory_usage())
# Since the maximum entry in humidity column is 100 and the dtype is int64, we can convert the dtype to int8 to save memory
yulu['humidity'] = yulu['humidity'].astype('int8')
print('Updated Memory usage of humidity column : ', yulu['humidity'].memory_usage())

"""####Modifying the dtype of the windspeed column"""

print('Max value entry in windspeed column : ', yulu['windspeed'].max())
print('Memory usage of windspeed column : ', yulu['windspeed'].memory_usage())
# Since the maximum entry in windspeed column is 56.9969 and the dtype is float64, we can convert the dtype to float32 to save memory
yulu['windspeed'] = yulu['windspeed'].astype('float32')
print('Updated Memory usage of windspeed column : ', yulu['windspeed'].memory_usage())

"""####Modifying the dtype of the casual column"""

print('Max value entry in casual column : ', yulu['casual'].max())
print('Memory usage of casual column : ', yulu['casual'].memory_usage())
# Since the maximum entry in casual column is 367 and the dtype is int64, we can convert the dtype to int16 to save memory
yulu['casual'] = yulu['casual'].astype('int16')
print('Updated Memory usage of casual column : ', yulu['casual'].memory_usage())

"""####Modifying the dtype of the registered column"""

print('Max value entry in registered column : ', yulu['registered'].max())
print('Memory usage of registered column : ', yulu['registered'].memory_usage())
# Since the maximum entry in registered column is 886 and the dtype is int64, we can convert the dtype to int16 to save memory
yulu['registered'] = yulu['registered'].astype('int16')
print('Updated Memory usage of registered column : ', yulu['registered'].memory_usage())

"""####Modifying the dtype of the count column"""

print('Max value entry in count column : ', yulu['count'].max())
print('Memory usage of count column : ', yulu['count'].memory_usage())
# Since the maximum entry in count column is 977 and the dtype is int64, we can convert the dtype to int16 to save memory
yulu['count'] = yulu['count'].astype('int16')
print('Updated Memory usage of count column : ', yulu['count'].memory_usage())

yulu.info()

"""The dataset originally consumed 1.1+ MB of memory, but now it has been compressed to 415.2+ KB, resulting in a substantial 63.17% reduction in memory usage.

###**Fundamental overview of the dataset**
"""

yulu.describe()

np.round(yulu['season'].value_counts(normalize = True) * 100, 2)

np.round(yulu['holiday'].value_counts(normalize = True) * 100, 2)

np.round(yulu['workingday'].value_counts(normalize = True) * 100, 2)

np.round(yulu['weather'].value_counts(normalize = True) * 100, 2)

# The below code generates a visually appealing pie chart to showcase the
    # distribution of seasons in the dataset

plt.figure(figsize = (6, 6))      # setting the figure size to 6*6

# setting the title of the plot
plt.title('Distribution of season', fontdict = {'fontsize' : 18,
                                                'fontweight' : 600,
                                                'fontstyle' : 'oblique',
                                                'fontfamily' : 'serif'})

df_season = np.round(yulu['season'].value_counts(normalize = True) * 100, 2).to_frame()

# Creating the pie-chart
plt.pie(x = df_season['season'],
        explode = [0.025, 0.025, 0.025, 0.025],
        labels = df_season.index,
        autopct = '%.2f%%',
        textprops = {'fontsize' : 14,
                   'fontstyle' : 'oblique',
                   'fontfamily' : 'serif',
                   'fontweight' : 500})

plt.plot()     # displaying the plot

"""* **Winter**: 25.11 - Winter has the highest value at 25.11.
* **Fall**: 25.11 - Fall also has a value of 25.11, tying with Winter for the highest.
* **Summer**: 25.11 - Summer shares the highest value of 25.11 with Winter and Fall.
* **Spring**: 24.67 - Spring has a slightly lower value of 24.67 compared to the other seasons
"""

# The below code generates a visually appealing pie chart to showcase the
    # distribution of holiday in the dataset

plt.figure(figsize = (6, 6))     # setting the figure size to 6*6

# setting the title of the plot
plt.title('Distribution of holiday', fontdict = {'fontsize' : 18,
                                                'fontweight' : 600,
                                                'fontstyle' : 'oblique',
                                                'fontfamily' : 'serif'})

df_holiday = np.round(yulu['holiday'].value_counts(normalize = True) * 100, 2).to_frame()

# Creating the pie-chart
plt.pie(x = df_holiday['holiday'],
        explode = [0, 0.1],
        labels = ['Non-Holiday', 'Holiday'],
        autopct = '%.2f%%',
        textprops = {'fontsize' : 14,
                   'fontstyle' : 'oblique',
                   'fontfamily' : 'serif',
                   'fontweight' : 500})

plt.plot()

"""**Non-Holiday** - The majority of the data falls into Non-Holiday, accounting for 97.14%.
**Holiday** - A small proportion of the data is in holiday, making up only 2.86%
"""

# The below code generates a visually appealing pie chart to showcase the
    # distribution of workingday in the dataset

plt.figure(figsize = (6, 6))    # setting the figure size to 6*6

# setting the title of the plot
plt.title('Distribution of workingday', fontdict = {'fontsize' : 18,
                                                'fontweight' : 600,
                                                'fontstyle' : 'oblique',
                                                'fontfamily' : 'serif'})

df_workingday = np.round(yulu['workingday'].value_counts(normalize = True) * 100, 2).to_frame()

# Creating the pie-chart
plt.pie(x = df_workingday['workingday'],
        explode = [0, 0.05],
        labels = ['Working Day', 'Non-Working Day'],
        autopct = '%.2f%%',
        textprops = {'fontsize' : 14,
                   'fontstyle' : 'oblique',
                   'fontfamily' : 'serif',
                   'fontweight' : 500})

plt.plot()         # displaying the plot

"""* **Working Day**- Most of the data belongs to category of working day, constituting 68.09% of the total.
* **Non-Working Day** - A smaller portion of the data is in non-working day, making up 31.91%.
"""

# The below code generates a visually appealing pie chart to showcase the
    # distribution of weather in the dataset

plt.figure(figsize = (6, 6))     # setting the figure size to 6*6

# setting the title of the plot
plt.title('Distribution of weather', fontdict = {'fontsize' : 18,
                                                'fontweight' : 600,
                                                'fontstyle' : 'oblique',
                                                'fontfamily' : 'serif'})

df_weather = np.round(yulu['weather'].value_counts(normalize = True) * 100, 2).to_frame()

# Creating the pie-chart
plt.pie(x = df_weather['weather'],
        explode = [0.025, 0.025, 0.05, 0.05],
        labels = df_weather.index,
        autopct = '%.2f%%',
        textprops = {'fontsize' : 14,
                   'fontstyle' : 'oblique',
                   'fontfamily' : 'serif',
                   'fontweight' : 500})

plt.plot()        # displaying the plot

"""* **1: 66.07** - The majority of the data is in category 1, making up 66.07%.
* **2: 26.03** - A significant proportion is in category 2, accounting for 26.03%.
* **3: 7.89** - A smaller portion is in category 3, making up 7.89%.
* **4: 0.01** - A very tiny fraction is in category 4, representing only 0.01%.

###**Univariate Analysis**
"""

# The below code generates a visually appealing count plot to showcase the
    # distribution of season in the dataset
sns.countplot(data = yulu, x = 'season')
plt.plot()

# The below code generates a visually appealing count plot to showcase the
    # distribution of holiday in the dataset

sns.countplot(data = yulu, x = 'holiday')
plt.plot()

# The below code generates a visually appealing count plot to showcase the
    # distribution of workingday in the dataset

sns.countplot(data = yulu, x = 'workingday')
plt.plot()

# The below code generates a visually appealing count plot to showcase the
    # distribution of weather in the dataset

sns.countplot(data = yulu, x = 'weather')
plt.plot()

# The below code generates a histogram plot for the 'temp' feature, showing the distribution of
    # temperature values in the dataset.
# The addition of the kernel density estimation plot provides
    # a visual representation of the underlying distribution shape, making it easier to analyze the
    # data distribution.

sns.histplot(data = yulu, x = 'temp', kde = True, bins = 40)
plt.plot()        # displaying the chart

temp_mean = np.round(yulu['temp'].mean(), 2)
temp_std = np.round(yulu['temp'].std(), 2)
temp_mean, temp_std

"""The average and the standard deviation of the 'temp' column are 20.23 and 7.79 degrees Celsius, respectively"""

# The below code generates a histogram plot for the 'temp' feature, showing the cumulative
    # distribution of temperature values in the dataset.
# The addition of the kernel density estimation plot provides
    # a visual representation of the underlying distribution shape, making it easier to analyze the
    # data distribution.

sns.histplot(data = yulu, x = 'temp', kde = True, cumulative = True, stat = 'percent')
plt.grid(axis = 'y', linestyle = '--')
plt.yticks(np.arange(0, 101, 10))
plt.plot()

"""In over 80% of the instances, the temperature remains below 28 degrees Celsius."""

# The below code generates a histogram plot for the 'atemp' feature, showing the distribution of
    # feeling temperature values in the dataset.
# The addition of the kernel density estimation plot provides
    # a visual representation of the underlying distribution shape, making it easier to analyze the
    # data distribution.

sns.histplot(data = yulu, x = 'atemp', kde = True, bins = 50)
plt.plot()

temp_mean = np.round(yulu['atemp'].mean(), 2)
temp_std = np.round(yulu['atemp'].std(), 2)
temp_mean, temp_std

"""The mean and standard deviation of the 'atemp' column are 23.66 and 8.47 degrees Celsius, respectively"""

# The below code generates a histogram plot for the 'humidity' feature, showing the distribution of
    # humidity values in the dataset.
# The addition of the kernel density estimation plot provides
    # a visual representation of the underlying distribution shape, making it easier to analyze the
    # data distribution.

sns.histplot(data = yulu, x = 'humidity', kde = True, bins = 50)
plt.plot()

humidity_mean = np.round(yulu['humidity'].mean(), 2)
humidity_std = np.round(yulu['humidity'].std(), 2)
humidity_mean, humidity_std

"""The average and standard deviation for the 'humidity' column are 61.89 and 19.25, respectively."""

# The below code generates a histogram plot for the 'humidity' feature, showing the cumulative
    # distribution of humidity values in the dataset.
# The addition of the kernel density estimation plot provides
    # a visual representation of the underlying distribution shape, making it easier to analyze the
    # data distribution.

sns.histplot(data = yulu, x = 'humidity', kde = True, cumulative = True, stat = 'percent')
plt.grid(axis = 'y', linestyle = '--')    # setting the gridlines along y axis
plt.yticks(np.arange(0, 101, 10))
plt.plot()

sns.histplot(data = yulu, x = 'windspeed', kde = True, cumulative = True, stat = 'percent')
plt.grid(axis = 'y', linestyle = '--')
plt.yticks(np.arange(0, 101, 10))
plt.plot()

len(yulu[yulu['windspeed'] < 20]) / len(yulu)

"""Percentage of windspeed data values less than 20: 86.27%"""

# The below code generates a histogram plot for the 'casual' feature, showing the distribution of
    # casual users' values in the dataset.
# The addition of the kernel density estimation plot provides
    # a visual representation of the underlying distribution shape, making it easier to analyze the
    # data distribution.

sns.histplot(data = yulu, x = 'casual', kde = True, bins = 50)
plt.plot()

sns.histplot(data = yulu, x = 'casual', kde = True, cumulative = True, stat = 'percent')
plt.grid(axis = 'y', linestyle = '--')
plt.yticks(np.arange(0, 101, 10))
plt.plot()

"""Over 80% of the time, the count of casual users remains below 60."""

# The below code generates a histogram plot for the 'registered' feature, showing the distribution of
    # registered users' values in the dataset.
# The addition of the kernel density estimation plot provides
    # a visual representation of the underlying distribution shape, making it easier to analyze the
    # data distribution.

sns.histplot(data = yulu, x = 'registered', kde = True, bins = 50)
plt.plot()

sns.histplot(data = yulu, x = 'registered', kde = True, cumulative = True, stat = 'percent')
plt.grid(axis = 'y', linestyle = '--')
plt.yticks(np.arange(0, 101, 10))
plt.plot()

"""In over 85% of instances, the count of registered users is below 300.

###**Outlier Detection**
"""

columns = ['temp', 'humidity', 'windspeed', 'casual', 'registered', 'count']
colors = np.random.permutation(['red', 'blue', 'green', 'magenta', 'cyan', 'gray'])
count = 1
plt.figure(figsize = (11, 14))
for i in columns:
    plt.subplot(3, 2, count)
    plt.title(f"Detecting outliers in '{i}' column")
    sns.boxplot(data = yulu, x = yulu[i], color = colors[count - 1], showmeans = True, fliersize = 2)
    plt.plot()
    count += 1

"""* No outliers are detected in the 'temp' column.
* A few outliers are identified in the 'humidity' column.
* Many outliers are observed in each of the following columns: 'windspeed,' 'casual,' 'registered,' and 'count.'

###**Bivariate Analysis**
"""

plt.figure(figsize = (15, 6))
plt.title('Distribution of hourly count of total rental bikes across all seasons',
         fontdict = {'size' : 20,
                    'style' : 'oblique',
                    'family' : 'serif'})
sns.boxplot(data = yulu, x = 'season', y = 'count', hue = 'workingday', showmeans = True)
plt.grid(axis = 'y', linestyle = '--')
plt.plot()

"""The hourly count of total rental bikes peaks during the fall season, with summer and winter following closely behind. In contrast, it tends to be relatively lower during the spring season."""

plt.figure(figsize = (15, 6))
plt.title('Distribution of hourly count of total rental bikes across all weathers',
         fontdict = {'size' : 20,
                    'style' : 'oblique',
                    'family' : 'serif'})
sns.boxplot(data = yulu, x = 'weather', y = 'count', hue = 'workingday', showmeans = True)
plt.grid(axis = 'y', linestyle = '--')
plt.plot()

"""The hourly count of total rental bikes is highest during clear and cloudy weather conditions, followed by misty weather and rainy weather. Records for extreme weather conditions are rare.

####Does the working day have an impact on the number of electric cycles rented?
"""

yulu.groupby(by = 'workingday')['count'].describe()

sns.boxplot(data = yulu, x = 'workingday', y = 'count')
plt.plot()

"""**Step 1**: Define Hypotheses

**Null Hypothesis (H0):** Working Day has no significant effect on the number of electric cycles rented.

**Alternative Hypothesis (HA):** Working Day does have a significant effect on the number of electric cycles rented.

**Step 2:** Assumption Checking:

* We perform a series of checks to meet the assumptions for the hypothesis:

* Check the distribution using a QQ Plot.

* Assess homogeneity of variances using Levene's test.

**Step 3:** Define Test Statistics under H0

* If the assumptions for the T Test are satisfied, we proceed with the T Test for independent samples. Otherwise, we perform the non-parametric equivalent of the T Test for independent samples, namely, the Mann-Whitney U rank test for two independent samples.

**Step 4:** Set Significance Level (Alpha)

* We set our alpha (significance level) to 0.05.

**Step 5:** Compare p-value and Alpha

Based on the computed p-value:

* If p-value > alpha, we accept the Null Hypothesis (H0).

* If p-value < alpha, we reject the Null Hypothesis (H0).

####Visual checks for normal distribution in samples.
"""

plt.figure(figsize = (15, 5))
plt.subplot(1, 2, 1)
sns.histplot(yulu.loc[yulu['workingday'] == 1, 'count'].sample(2000),
             element = 'step', color = 'green', kde = True, label = 'workingday')
plt.legend()
plt.subplot(1, 2, 2)
sns.histplot(yulu.loc[yulu['workingday'] == 0, 'count'].sample(2000),
             element = 'step', color = 'blue', kde = True, label = 'non_workingday')
plt.legend()
plt.plot()

"""From the plot above, it's evident that the distributions do not resemble a normal distribution.

####Distribution check using QQ Plot
"""

plt.figure(figsize = (15, 6))
plt.subplot(1, 2, 1)
plt.suptitle('QQ plots for the count of electric vehicles rented in workingday and non_workingday')
spy.probplot(yulu.loc[yulu['workingday'] == 1, 'count'].sample(2000), plot = plt, dist = 'norm')
plt.title('QQ plot for workingday')
plt.subplot(1, 2, 2)
spy.probplot(yulu.loc[yulu['workingday'] == 0, 'count'].sample(2000), plot = plt, dist = 'norm')
plt.title('QQ plot for non_workingday')
plt.plot()

"""The plot above indicates that the distributions do not conform to a normal distribution.

####From the plots above, it's apparent that the samples deviate from a normal distribution.

Shapiro-Wilk Test for Normality

**Null Hypothesis (H0):** The sample follows a normal distribution.

**Alternative Hypothesis (H1):** The sample does not follow a normal distribution.

**Significance Level (Alpha)**: 0.05


**Test Statistics:** Shapiro-Wilk Test for Normality
"""

test_stat, p_value = spy.shapiro(yulu.loc[yulu['workingday'] == 1, 'count'].sample(2000))
print('p-value', p_value)
if p_value < 0.05:
    print('The sample does not follow normal distribution')
else:
    print('The sample follows normal distribution')

test_stat, p_value = spy.shapiro(yulu.loc[yulu['workingday'] == 0, 'count'].sample(2000))
print('p-value', p_value)
if p_value < 0.05:
    print('The sample does not follow normal distribution')
else:
    print('The sample follows normal distribution')

"""####Applying a Box-Cox transformation to the data and assessing whether the transformed data conforms to a normal distribution."""

transformed_workingday = spy.boxcox(yulu.loc[yulu['workingday'] == 1, 'count'])[0]
test_stat, p_value = spy.shapiro(transformed_workingday)
print('p-value', p_value)
if p_value < 0.05:
    print('The sample does not follow normal distribution')
else:
    print('The sample follows normal distribution')

transformed_non_workingday = spy.boxcox(yulu.loc[yulu['workingday'] == 1, 'count'])[0]
test_stat, p_value = spy.shapiro(transformed_non_workingday)
print('p-value', p_value)
if p_value < 0.05:
    print('The sample does not follow normal distribution')
else:
    print('The sample follows normal distribution')

"""Even with the application of the Box-Cox transformation to both "workingday" and "non_workingday" data, it is evident that the samples still do not exhibit a normal distribution.

####Are the rental counts of cycles similar or different in various seasons?
"""

yulu.groupby(by = 'season')['count'].describe()

yulu_season_spring = yulu.loc[yulu['season'] == 'spring', 'count']
yulu_season_summer = yulu.loc[yulu['season'] == 'summer', 'count']
yulu_season_fall = yulu.loc[yulu['season'] == 'fall', 'count']
yulu_season_winter = yulu.loc[yulu['season'] == 'winter', 'count']
len(yulu_season_spring), len(yulu_season_summer), len(yulu_season_fall), len(yulu_season_winter)

sns.boxplot(data = yulu, x = 'season', y = 'count', showmeans = True)
plt.plot()

"""**Step 1:** Formulate Hypotheses

**Null Hypothesis (H0):** The mean of cycle rentals per hour is the same for all four seasons (Season 1, Season 2, Season 3, and Season 4).

**Alternative Hypothesis (HA):** The mean of cycle rentals per hour differs among the seasons.


**Step 2:** Assumption Checks

Examine the normality of the data using QQ Plots. If the data distribution is not normal, consider applying the Box-Cox transformation to achieve normality.
Evaluate the homogeneity of variances using Levene's test.
Ensure the independence of each observation.


**Step 3:** Define Test Statistics

The test statistic for a one-way ANOVA is denoted as F. In the context of independent variable with 'k' groups, the F statistic assesses whether the group means are statistically different.

F = MSB / MSW

Under the null hypothesis (H0), the test statistic follows the F-distribution.


**Step 4**: Choose the Test Type

In this analysis, we will be conducting a right-tailed F-test.


**Step 5:** Calculate p-Value and Set Alpha

Calculate the p-value for the ANOVA test using the f_oneway function from scipy.stats. Set the significance level (alpha) to 0.05.


**Step 6:** Compare p-Value and Alpha

Based on the p-value, you will decide whether to accept or reject the null hypothesis:

If p-value > alpha, accept H0.

If p-value < alpha, reject H0.

The one-way ANOVA aims to determine if there are significant differences in means between the groups being compared. If the ANOVA test yields a statistically significant result, it suggests that at least two group means are significantly different from each other, thus rejecting the null hypothesis.

**Visual checks for normal distribution in samples.**
"""

plt.figure(figsize = (12, 6))
plt.subplot(2, 2, 1)
sns.histplot(yulu_season_spring.sample(2500), bins = 50,
             element = 'step', color = 'green', kde = True, label = 'season_spring')
plt.legend()
plt.subplot(2, 2, 2)
sns.histplot(yulu_season_summer.sample(2500), bins = 50,
             element = 'step', color = 'blue', kde = True, label = 'season_summer')
plt.legend()
plt.subplot(2, 2, 3)
sns.histplot(yulu_season_fall.sample(2500), bins = 50,
             element = 'step', color = 'red', kde = True, label = 'season_fall')
plt.legend()
plt.subplot(2, 2, 4)
sns.histplot(yulu_season_winter.sample(2500), bins = 50,
             element = 'step', color = 'yellow', kde = True, label = 'season_winter')
plt.legend()
plt.plot()

"""The plot above suggests that the distributions do not adhere to a normal distribution.

**Distribution check using QQ Plot**
"""

plt.figure(figsize = (12, 12))
plt.subplot(2, 2, 1)
plt.suptitle('QQ plots for the count of electric vehicles rented in different seasons')
spy.probplot(yulu_season_spring.sample(2500), plot = plt, dist = 'norm')
plt.title('QQ plot for spring season')

plt.subplot(2, 2, 2)
spy.probplot(yulu_season_summer.sample(2500), plot = plt, dist = 'norm')
plt.title('QQ plot for summer season')

plt.subplot(2, 2, 3)
spy.probplot(yulu_season_fall.sample(2500), plot = plt, dist = 'norm')
plt.title('QQ plot for fall season')

plt.subplot(2, 2, 4)
spy.probplot(yulu_season_winter.sample(2500), plot = plt, dist = 'norm')
plt.title('QQ plot for winter season')
plt.plot()

"""It is evident from the above plots that the distributions do not adhere to a normal distribution.

Shapiro-Wilk Test for Normality

**Null Hypothesis (H0):** The sample follows a normal distribution.

**Alternative Hypothesis (H1):** The sample does not follow a normal distribution.

**Significance Level (Alpha):** 0.05

**Test Statistics:** Shapiro-Wilk Test for Normality
"""

test_stat, p_value = spy.shapiro(yulu_season_spring.sample(2500))
print('p-value', p_value)
if p_value < 0.05:
    print('The sample does not follow normal distribution')
else:
    print('The sample follows normal distribution')

test_stat, p_value = spy.shapiro(yulu_season_summer.sample(2500))
print('p-value', p_value)
if p_value < 0.05:
    print('The sample does not follow normal distribution')
else:
    print('The sample follows normal distribution')

test_stat, p_value = spy.shapiro(yulu_season_fall.sample(2500))
print('p-value', p_value)
if p_value < 0.05:
    print('The sample does not follow normal distribution')
else:
    print('The sample follows normal distribution')

test_stat, p_value = spy.shapiro(yulu_season_winter.sample(2500))
print('p-value', p_value)
if p_value < 0.05:
    print('The sample does not follow normal distribution')
else:
    print('The sample follows normal distribution')

# Null Hypothesis(H0) - Homogenous Variance

# Alternate Hypothesis(HA) - Non Homogenous Variance

test_stat, p_value = spy.levene(yulu_season_spring.sample(2500),
                                yulu_season_summer.sample(2500),
                                yulu_season_fall.sample(2500),
                                yulu_season_winter.sample(2500))
print('p-value', p_value)
if p_value < 0.05:
    print('The samples do not have  Homogenous Variance')
else:
    print('The samples have Homogenous Variance ')

"""####As the samples do not exhibit a normal distribution and do not possess equal variances, the f_oneway test is not applicable in this context. Instead, we can perform its non-parametric equivalent test, the Kruskal-Wallis H-test for independent samples."""

# Ho : Mean no. of cycles rented is same for different weather
# Ha : Mean no. of cycles rented is different for different weather
# Assuming significance Level to be 0.05
alpha = 0.05
test_stat, p_value = spy.kruskal(yulu_season_spring, yulu_season_summer, yulu_season_fall, yulu_season_winter)
print('Test Statistic =', test_stat)
print('p value =', p_value)

# Comparing p value with significance level

if p_value < alpha:
    print('Reject Null Hypothesis')
else:
    print('Failed to reject Null Hypothesis')

"""####Statistically, the mean number of rental bikes varies significantly across different seasons."""

sns.pairplot(data = yulu,
             kind = 'reg',
             hue = 'workingday',
             markers = '.')
plt.plot()

corr_data = yulu.corr()
corr_data

plt.figure(figsize = (12, 8))
sns.heatmap(data = corr_data, cmap = 'Greens', annot = True, vmin = -1, vmax = 1)
plt.plot()

"""###**Insights:**

**Data Duration**:

The dataset spans from January 1, 2011, to December 19, 2012, covering 718 days and 23 hours.


**User Composition:**

Out of every 100 users, around 19 are casual users, while 81 are registered users.


**Yearly Growth:**

The mean total hourly count of rental bikes increased from 144 in 2011 to 239 in 2012, showing an annual growth rate of 65.41%.


**Seasonal Demand:**

There's a seasonal pattern in the count of rental bikes, with higher demand in spring and summer, a slight decline in fall, and a further decrease in the winter months.


**Monthly Variations:**

The lowest average hourly count of rental bikes is observed in January, followed by February and March.


**Daily Fluctuations:**

The dataset exhibits distinct daily fluctuations, with low counts in the early morning, a morning surge, peak counts in the afternoon, and gradual declines in the evening and nighttime.


**Temperature Distribution:**

Over 80% of the time, the temperature remains below 28 degrees Celsius.


**Humidity Levels:**

More than 80% of the time, the humidity level is greater than 40, indicating variations between optimal and slightly moist conditions.


**Windspeed Data:**

Over 85% of the total windspeed data has values less than 20.


**Weather Impact:**

The highest hourly count of total rental bikes is observed during clear and cloudy weather, followed by misty weather and rainy weather. There are very few records for extreme weather conditions.


**Working Day Influence:**

The mean hourly count of total rental bikes is statistically similar for both working and non-working days.


**Weather-Season Relationship:**

There is a statistically significant relationship between weather and season based on the hourly total number of rented bikes.


**Weather Dependency:**

The hourly total number of rental bikes varies significantly among different weather conditions.


**Weather-Season Independence:**

However, there's no statistically significant dependency of weather categories (1, 2, 3) on the season concerning the average hourly total number of rented bikes.


**Seasonal Differences:**

The hourly total number of rental bikes is statistically different for different seasons.

###**Recommandations:**

**Seasonal Marketing:**

Capitalize on the clear seasonal pattern in bike rentals. Tailor marketing efforts to boost bike rentals during spring and summer, offering seasonal discounts and special packages to entice customers during high-demand periods.


**Time-based Pricing:**

Implement dynamic pricing based on hourly fluctuations. Lower rental rates during off-peak hours and increase rates during peak times. This strategy can distribute demand more evenly throughout the day and optimize resource utilization.


**Weather-based Promotions:**

Recognize the influence of weather conditions on bike rentals. Develop targeted promotions for clear and cloudy weather, the periods with the highest rental counts. Offer weather-specific discounts to attract more customers during favorable weather conditions.


**User Segmentation:**

Leverage the distinction between registered (81%) and casual (19%) users. Create loyalty programs, exclusive offers, or personalized recommendations for registered users to foster repeat business. For casual users, emphasize the convenience and benefits of bike rentals for occasional use.


**Optimize Inventory:**

Analyze demand patterns across different months and adjust inventory accordingly. During months with lower rental counts, optimize inventory to avoid excess bikes. During peak months, ensure an adequate supply to meet heightened demand.


**Enhance Weather Data Collection:**

Improve data collection for extreme weather conditions to gain insights into customer behavior during adverse weather. This data can guide decisions regarding specialized bike models, safety measures, and targeted marketing.


**Customer Comfort:**

Given high humidity levels and sub-28°C temperatures, enhance customer comfort. Provide amenities like umbrellas, rain jackets, or water bottles to improve the biking experience and encourage customer satisfaction.


**Collaborations with Weather Services:**

Partner with weather services to provide real-time updates and forecasts in marketing campaigns and rental apps. Highlight ideal biking conditions to attract users who prefer specific weather scenarios.


**Seasonal Bike Maintenance:**

Allocate resources for seasonal bike maintenance. Conduct thorough checks before peak seasons to ensure bikes are in top condition. Regular inspections and servicing throughout the year can prevent breakdowns and enhance customer satisfaction.


**Customer Feedback and Reviews:**

Encourage customers to provide feedback and reviews to identify areas for improvement, understand preferences, and tailor services to meet customer expectations.


**Social Media Marketing:**

Utilize social media platforms to promote electric bike rentals. Showcase biking experiences in various weather conditions, share customer testimonials, and engage with potential customers. Use targeted advertising campaigns to reach specific customer segments.


**Special Occasion Discounts:**

Align Yulu's focus on sustainable solutions with special discounts on environmentally significant occasions like Zero Emissions Day, Earth Day, and World Environment Day. Attract new users who share these environmental values.
"""