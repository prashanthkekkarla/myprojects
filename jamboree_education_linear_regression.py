# -*- coding: utf-8 -*-
"""Jamboree Education - Linear Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_1Ue2OwPVOb-8IuH6iVScm9pTNMJ39HD

##**Jamboree Education - Linear Regression**

####**Context**

Jamboree has helped thousands of students like you make it to top colleges abroad. Be it GMAT, GRE or SAT, their unique problem-solving methods ensure maximum scores with minimum effort.

They recently launched a feature where students/learners can come to their website and check their probability of getting into the IVY league college. This feature estimates the chances of graduate admission from an Indian perspective

####**How can you help here?**

Your analysis will help Jamboree in understanding what factors are important in graduate admissions and how these factors are interrelated among themselves. It will also help predict one's chances of admission given the rest of the variables.

####**Column Profiling:**

*Serial No. (Unique row ID)

*GRE Scores (out of 340)

*TOEFL Scores (out of 120)

*University Rating (out of 5)

*Statement of Purpose and Letter of Recommendation Strength (out of 5)

*Undergraduate GPA (out of 10)

*Research Experience (either 0 or 1)

*Chance of Admit (ranging from 0 to 1)

####Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from matplotlib import figure

import warnings
warnings.filterwarnings('ignore')

import statsmodels.api as sm

"""####Loading Dataset"""

Data = pd.read_csv("https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/001/839/original/Jamboree_Admission.csv")

Data.head()

Data.shape

df = Data.copy()

# dropping first  not required column "Serial No."

df.drop(["Serial No."],axis=1,inplace=True)

df.isna().sum()               # null values check

df.info()

df.columns

df.columns  = ['GRE_Score', 'TOEFL_Score', 'University_Rating', 'SOP', 'LOR', 'CGPA',
       'Research', 'Chance_of_Admit']

       # changing / removing space between column names.

df.sample(2)

df.nunique()

"""* The variables University Rating, SOP, LOR, and Research appear to be categorical, given their limited number of unique values. They are discrete in nature. The remaining features are primarily numeric and ordinal.

* Furthermore, it's worth considering whether SOP, University Rating, LOR, and Research could be treated as numeric ordinal data.

###Exploring the general linearity and correlation across all features through a pairplot:
"""

sns.pairplot(df.corr(),kind= 'reg')

"""###An overview of the correlation among all variables:"""

plt.figure(figsize=(9,7))
sns.heatmap(df.corr(),annot=True,cmap = "Reds")

"""* Independent Variables (Input data): GRE Score, TOEFL Score, University Rating, SOP, LOR, CGPA, Research

* Target/Dependent Variable: Chance of Admit (the value we want to predict)

* From the correlation heatmap, it's evident that GRE Score, TOEFL Score, and CGPA exhibit a notably strong correlation with the Chance of Admit.

* On the other hand, University Rating, SOP, LOR, and Research show relatively lower correlations compared to the other features.

###Outliers within the dataset
"""

def detect_outliers(data):
    length_before = len(data)
    Q1 = np.percentile(data,25)
    Q3 = np.percentile(data,75)
    IQR = Q3-Q1
    upperbound = Q3+1.5*IQR
    lowerbound = Q1-1.5*IQR
    if lowerbound < 0:
        lowerbound = 0

    length_after = len(data[(data>lowerbound)&(data<upperbound)])
    return f"{np.round((length_before-length_after)/length_before,4)} % Outliers data from input data found"

for col in df.columns:
    print(col," : ",detect_outliers(df[col]))

detect_outliers(df)

"""* There are no significant outliers detected in the dataset.

###Summary statistics of all numeric features:
"""

df.describe()

"""* The Chance of Admit is a probability measure, ranging from 0 to 1, which indicates favorable data distribution without outliers or misleading entries in the column.

* The GRE score ranges from 290 to 340, while the TOEFL score ranges from 92 to 120.

* University Rating, SOP, and LOR fall within the range of 1 to 5.

* CGPA spans from 6.8 to 9.92.

##Graphical Analysis :

###Visual representations of distributions through histograms and count plots:

###Chance_of_Admit
"""

sns.distplot(df["Chance_of_Admit"],bins = 30)
sm.qqplot(df["Chance_of_Admit"],fit=True, line="45")
plt.show()

"""###GRE_Score"""

sns.distplot(df["GRE_Score"], bins = 30)
sm.qqplot(df["GRE_Score"],fit=True, line="45")
plt.show()

Data["GRE_SCORE_CATEGORY"]=pd.qcut(Data["GRE Score"],20)
plt.figure(figsize=(14,5))
sns.boxplot(y = Data["Chance of Admit "], x = Data["GRE_SCORE_CATEGORY"])
plt.xticks(rotation = 90)
plt.show()

"""*
Based on the boxplot illustrating the distribution of admission chances concerning GRE scores, it appears that a higher GRE score correlates with a higher probability of admission.
"""

sns.jointplot(x="GRE_Score", y="Chance_of_Admit", data=df, kind="reg")

"""###TOEFL_Score"""

sns.distplot(df["TOEFL_Score"])
sm.qqplot(df["TOEFL_Score"],fit=True, line="45")
plt.show()
plt.figure(figsize=(14,5))
sns.boxplot(y = df["Chance_of_Admit"], x = df["TOEFL_Score"])

"""*
Students with higher TOEFL scores tend to have a higher probability of admission

###CGPA
"""

sns.distplot(df["CGPA"], bins = 30)
sm.qqplot(df["CGPA"],fit=True, line="45")
plt.show()

"""*
The distribution of Chance of Admission and GRE scores appears to be approximately normal.

###Distribution of all remaining categorical features:
"""

# Convert columns to categorical type if they are not already
df["University_Rating"] = df["University_Rating"].astype("category")
df["LOR"] = df["LOR"].astype("category")
df["SOP"] = df["SOP"].astype("category")
df["Research"] = df["Research"].astype("category")

plt.figure(figsize=(10, 7))
custom_palette = sns.color_palette("Paired")

plt.subplot(2, 2, 1)
sns.countplot(data=df, x="University_Rating", palette=custom_palette)
plt.xlabel("University Rating")

plt.subplot(2, 2, 2)
sns.countplot(data=df, x="LOR", palette=custom_palette)
plt.xlabel("LOR")

plt.subplot(2, 2, 3)
sns.countplot(data=df, x="SOP", palette=custom_palette)
plt.xlabel("SOP")

plt.subplot(2, 2, 4)
sns.countplot(data=df, x="Research", palette=custom_palette)
plt.xlabel("Research")

plt.tight_layout()  # Adjust subplots to prevent overlap
plt.show()

sns.pairplot(df, y_vars=["Chance_of_Admit"], x_vars=["University_Rating", "LOR", "SOP", "Research", "GRE_Score", "TOEFL_Score", "CGPA"])
plt.title("Pair plot Chance of admit vs all the features")
plt.show()

"""###Boxplot comparing categorical features with the Chance of Admit:"""

plt.figure(figsize=(12,8))
plt.subplot(2,2,1)
sns.boxplot(y = df["Chance_of_Admit"], x = df["SOP"])
plt.subplot(2,2,2)
sns.boxplot(y = df["Chance_of_Admit"], x = df["LOR"])
plt.subplot(2,2,3)
sns.boxplot(y = df["Chance_of_Admit"], x = df["University_Rating"])
plt.subplot(2,2,4)
sns.boxplot(y = df["Chance_of_Admit"], x = df["Research"])
plt.show()

"""* From the aforementioned plots, it's evident that the strength of the Statement of Purpose (SOP) is positively correlated with the Chance of Admission. A similar trend is observed with the strength of the Letter of Recommendation (LOR) and University Rating, both showing a positive correlation with the Chance of Admission.

* Furthermore, students involved in research exhibit higher chances of admission. However, it's notable that there are some outliers within this category.

###Linearity: Examining the correlation between features and the target variable, Chance of Admission:
"""

# Convert "University_Rating" to an ordered categorical type
df["University_Rating"] = pd.Categorical(df["University_Rating"], ordered=True)

# Iterate over each column
for col in df.columns:
    # For numerical columns, create a jointplot
    if df[col].dtype in ['int64', 'float64']:
        plt.figure(figsize=(6, 4))
        sns.jointplot(x=df[col], y=df["Chance_of_Admit"], kind="reg")
        plt.title(f"Joint plot: {col} vs Chance_of_Admit")
        plt.show()
    # For categorical columns, create other types of plots (e.g., countplot)
    else:
        plt.figure(figsize=(6, 4))
        sns.countplot(data=df, x=col)
        plt.title(f"Count plot: {col}")
        plt.show()

"""###Linear Regression"""

from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

from statsmodels.stats.outliers_influence import variance_inflation_factor

from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error, adjusted_mutual_info_score
from sklearn.feature_selection import f_regression

X = df.drop(["Chance_of_Admit"],axis = 1)  # independent variables
y = df["Chance_of_Admit"].values.reshape(-1,1) # target / dependent variables

"""####Data Standardization"""

standardizer = StandardScaler()
standardizer.fit(X)
x = standardizer.transform(X)  # standardising the data

"""###The train-test split"""

X_train , X_test, y_train , y_test = train_test_split(x,y, random_state = 1,test_size = 0.2 )

X_train.shape,X_test.shape

y_train.shape, y_test.shape

"""###LinearRegression model"""

LinearRegression = LinearRegression()
LinearRegression.fit(X_train,y_train)

"""###R2 score on train data"""

r2_score(y_train,LinearRegression.predict(X_train))

"""###R2 score on test data"""

r2_score(y_test,LinearRegression.predict(X_test) )

"""###Coefficients and intercept for all features:"""

ws = pd.DataFrame(LinearRegression.coef_.reshape(1,-1),columns=df.columns[:-1])
ws["Intercept"] = LinearRegression.intercept_
ws

LinearRegression_Model_coefs = ws
LinearRegression_Model_coefs

def AdjustedR2score(R2,n,d):
    return 1-(((1-R2)*(n-1))/(n-d-1))

y_pred = LinearRegression.predict(X_test)

print("MSE:",mean_squared_error(y_test,y_pred)) # MSE
print("RMSE:",np.sqrt(mean_squared_error(y_test,y_pred))) #RMSE
print("MAE :",mean_absolute_error(y_test,y_pred) ) # MAE
print("r2_score:",r2_score(y_test,y_pred)) # r2score
print("Adjusted R2 score :", AdjustedR2score(r2_score(y_test,y_pred),len(X),X.shape[1]))  # adjusted R2 score

"""###Assumptions of linear regression include:
* Absence of multicollinearity.
* Residuals having a mean close to zero.
* Linearity of variables.
* Homoscedasticity test.
* Normality of residuals.

###Multicollinearity check by VIF score
"""

vifs = []

for i in range(X_train.shape[1]):

    vifs.append((variance_inflation_factor(exog = X_train,
                                   exog_idx=i)))
vifs

pd.DataFrame({ "coef_name : " : X.columns ,
             "vif : ": np.around(vifs,2)})

"""* Since all Variance Inflation Factor (VIF) scores are below 5, there doesn't appear to be a significant issue with multicollinearity

###Residual Analysis
"""

y_predicted = LinearRegression.predict(X_train)
y_predicted.shape

from scipy import stats

residuals = (y_train - y_predicted)
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
sns.distplot(residuals)
plt.subplot(1,2,2)
stats.probplot(residuals.reshape(-1,), plot = plt)
plt.title('QQ plot for residual')
plt.show()

"""###Linearity of varibales"""

sns.pairplot(df, y_vars=["Chance_of_Admit"], x_vars=["University_Rating", "LOR", "SOP", "Research", "GRE_Score", "TOEFL_Score", "CGPA"])
plt.show()

# Test of homoscedasticity
sns.scatterplot(x=y_predicted.reshape(-1,), y=residuals.reshape(-1,))
plt.xlabel('y_predicted')
plt.ylabel('Residuals')
plt.axhline(y=0)
plt.title("Y_predicted vs residuals, check of homoscedasticity")
plt.show()

"""##Model Regularisation :"""

from sklearn.linear_model import Ridge  # L2 regualrization
from sklearn.linear_model import Lasso  # L1 regualrization
from sklearn.linear_model import ElasticNet

"""###L2 regularization
###Ridge regression :
"""

## Hyperparameter Tuning : for appropriate lambda value :

train_R2_score = []
test_R2_score = []
lambdas = []
train_test_difference_Of_R2 =  []
lambda_ = 0
while lambda_ <= 5:
    lambdas.append(lambda_)
    RidgeModel = Ridge(lambda_)
    RidgeModel.fit(X_train,y_train)
    trainR2 = RidgeModel.score(X_train,y_train)
    testR2 = RidgeModel.score(X_test,y_test)
    train_R2_score.append(trainR2)
    test_R2_score.append(testR2)

    lambda_ += 0.01

plt.figure(figsize=(8, 8))
sns.lineplot(x=lambdas, y=train_R2_score)
sns.lineplot(x=lambdas, y=test_R2_score)
plt.legend(['Train R2 Score', 'Test R2 score'])
plt.title("Effect of hyperparameter alpha on R2 scores of Train and test")
plt.show()

RidgeModel = Ridge(alpha = 0.1)
RidgeModel.fit(X_train,y_train)
trainR2 = RidgeModel.score(X_train,y_train)
testR2 = RidgeModel.score(X_test,y_test)

trainR2,testR2

RidgeModel.coef_

RidgeModel_coefs = pd.DataFrame(RidgeModel.coef_.reshape(1,-1),columns=df.columns[:-1])
RidgeModel_coefs["Intercept"] = RidgeModel.intercept_
RidgeModel_coefs

LinearRegression_Model_coefs

y_pred = RidgeModel.predict(X_test)

print("MSE:",mean_squared_error(y_test,y_pred)) # MSE
print("RMSE:",np.sqrt(mean_squared_error(y_test,y_pred))) #RMSE
print("MAE :",mean_absolute_error(y_test,y_pred) ) # MAE
print("r2_score:",r2_score(y_test,y_pred)) # r2score
print("Adjusted R2 score :", AdjustedR2score(r2_score(y_test,y_pred),len(X),X.shape[1]))  # adjusted R2 score

y_predicted = RidgeModel.predict(X_train)

residuals = (y_train - y_predicted)
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
sns.distplot(residuals)
plt.subplot(1,2,2)
stats.probplot(residuals.reshape(-1,), plot = plt)
plt.title('QQ plot for residual')
plt.show()

"""###L1 regularization :
###Lasso :
"""

## Hyperparameter Tuning : for appropriate lambda value :

train_R2_score = []
test_R2_score = []
lambdas = []
train_test_difference_Of_R2 =  []
lambda_ = 0
while lambda_ <= 5:
    lambdas.append(lambda_)
    LassoModel = Lasso(alpha=lambda_)
    LassoModel.fit(X_train , y_train)
    trainR2 = LassoModel.score(X_train,y_train)
    testR2 = LassoModel.score(X_test,y_test)
    train_R2_score.append(trainR2)
    test_R2_score.append(testR2)

    lambda_ += 0.001

plt.figure(figsize=(8, 8))
sns.lineplot(x=lambdas, y=train_R2_score)
sns.lineplot(x=lambdas, y=test_R2_score)
plt.legend(['Train R2 Score', 'Test R2 score'])
plt.title("Effect of hyperparameter alpha on R2 scores of Train and test")
plt.show()

LassoModel = Lasso(alpha=0.001)
LassoModel.fit(X_train , y_train)
trainR2 = LassoModel.score(X_train,y_train)
testR2 = LassoModel.score(X_test,y_test)

trainR2,testR2

Lasso_Model_coefs = pd.DataFrame(LassoModel.coef_.reshape(1,-1),columns=df.columns[:-1])
Lasso_Model_coefs["Intercept"] = LassoModel.intercept_
Lasso_Model_coefs

RidgeModel_coefs

LinearRegression_Model_coefs

y_predicted = LassoModel.predict(X_train)

residuals = (y_train - y_predicted)
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
sns.distplot(residuals)
plt.subplot(1,2,2)
stats.probplot(residuals.reshape(-1,), plot = plt)
plt.title('QQ plot for residual')
plt.show()

y_pred = LassoModel.predict(X_test)

print("MSE:",mean_squared_error(y_test,y_pred)) # MSE
print("RMSE:",np.sqrt(mean_squared_error(y_test,y_pred))) #RMSE
print("MAE :",mean_absolute_error(y_test,y_pred) ) # MAE
print("r2_score:",r2_score(y_test,y_pred)) # r2score
print("Adjusted R2 score :", AdjustedR2score(r2_score(y_test,y_pred),len(X),X.shape[1]))  # adjusted R2 score

"""###ElasticNet
###L1 and L2 regularisation :
Elastic Net regression utilizes both L1 and L2 regularization techniques, blending the penalties from Lasso and Ridge methods to regularize regression models.
"""

## Hyperparameter Tuning : for appropriate lambda value :

train_R2_score = []
test_R2_score = []
lambdas = []
train_test_difference_Of_R2 =  []
lambda_ = 0
while lambda_ <= 5:
    lambdas.append(lambda_)
    ElasticNet_model = ElasticNet(alpha=lambda_)
    ElasticNet_model.fit(X_train , y_train)
    trainR2 = ElasticNet_model.score(X_train,y_train)
    testR2 = ElasticNet_model.score(X_test,y_test)
    train_R2_score.append(trainR2)
    test_R2_score.append(testR2)

    lambda_ += 0.001

plt.figure(figsize=(10, 10))
sns.lineplot(x=lambdas, y=train_R2_score)
sns.lineplot(x=lambdas, y=test_R2_score)
plt.legend(['Train R2 Score', 'Test R2 score'])
plt.title("Effect of hyperparameter alpha on R2 scores of Train and test")
plt.show()

ElasticNet_model = ElasticNet(alpha=0.001)
ElasticNet_model.fit(X_train , y_train)
trainR2 = ElasticNet_model.score(X_train,y_train)
testR2 = ElasticNet_model.score(X_test,y_test)

trainR2,testR2

y_predicted = ElasticNet_model.predict(X_train)

residuals = (y_train - y_predicted)
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
sns.distplot(residuals)
plt.subplot(1,2,2)
stats.probplot(residuals.reshape(-1,), plot = plt)
plt.title('QQ plot for residual')
plt.show()

y_pred = ElasticNet_model.predict(X_test)

print("MSE:",mean_squared_error(y_test,y_pred)) # MSE
print("RMSE:",np.sqrt(mean_squared_error(y_test,y_pred))) #RMSE
print("MAE :",mean_absolute_error(y_test,y_pred) ) # MAE
print("r2_score:",r2_score(y_test,y_pred)) # r2score
print("Adjusted R2 score :", AdjustedR2score(r2_score(y_test,y_pred),len(X),X.shape[1]))  # adjusted R2 score

ElasticNet_model_coefs = pd.DataFrame(ElasticNet_model.coef_.reshape(1,-1),columns=df.columns[:-1])
ElasticNet_model_coefs["Intercept"] = ElasticNet_model.intercept_
ElasticNet_model_coefs

RidgeModel_coefs

Lasso_Model_coefs

LinearRegression_Model_coefs

y_pred = ElasticNet_model.predict(X_test)
ElasticNet_model_metrics = []
ElasticNet_model_metrics.append(mean_squared_error(y_test,y_pred)) # MSE
ElasticNet_model_metrics.append(np.sqrt(mean_squared_error(y_test,y_pred))) #RMSE
ElasticNet_model_metrics.append(mean_absolute_error(y_test,y_pred) ) # MAE
ElasticNet_model_metrics.append(r2_score(y_test,y_pred)) # r2score
ElasticNet_model_metrics.append(AdjustedR2score(r2_score(y_test,y_pred),len(X),X.shape[1]))  # adjusted R2 score

y_pred = LinearRegression.predict(X_test)
LinearRegression_model_metrics = []
LinearRegression_model_metrics.append(mean_squared_error(y_test,y_pred)) # MSE
LinearRegression_model_metrics.append(np.sqrt(mean_squared_error(y_test,y_pred))) #RMSE
LinearRegression_model_metrics.append(mean_absolute_error(y_test,y_pred) ) # MAE
LinearRegression_model_metrics.append(r2_score(y_test,y_pred)) # r2score
LinearRegression_model_metrics.append(AdjustedR2score(r2_score(y_test,y_pred),len(X),X.shape[1]))  # adjusted R2 score

y_pred = RidgeModel.predict(X_test)
RidgeModel_model_metrics = []
RidgeModel_model_metrics.append(mean_squared_error(y_test,y_pred)) # MSE
RidgeModel_model_metrics.append(np.sqrt(mean_squared_error(y_test,y_pred))) #RMSE
RidgeModel_model_metrics.append(mean_absolute_error(y_test,y_pred) ) # MAE
RidgeModel_model_metrics.append(r2_score(y_test,y_pred)) # r2score
RidgeModel_model_metrics.append(AdjustedR2score(r2_score(y_test,y_pred),len(X),X.shape[1]))  # adjusted R2 score

y_pred = LassoModel.predict(X_test)
LassoModel_model_metrics = []
LassoModel_model_metrics.append(mean_squared_error(y_test,y_pred)) # MSE
LassoModel_model_metrics.append(np.sqrt(mean_squared_error(y_test,y_pred))) #RMSE
LassoModel_model_metrics.append(mean_absolute_error(y_test,y_pred) ) # MAE
LassoModel_model_metrics.append(r2_score(y_test,y_pred)) # r2score
LassoModel_model_metrics.append(AdjustedR2score(r2_score(y_test,y_pred),len(X),X.shape[1]))  # adjusted R2 score

ElasticNet_model_metrics

A = pd.DataFrame([LinearRegression_model_metrics,LassoModel_model_metrics,RidgeModel_model_metrics,ElasticNet_model_metrics],columns=["MSE","RMSE","MAE","R2_SCORE","ADJUSTED_R2"],index = ["Linear Regression Model","Lasso Regression Model","Ridge Regression Model","ElasticNet Regression Model"])
A

B = pd.DataFrame(LinearRegression_Model_coefs.append(Lasso_Model_coefs).append(RidgeModel_coefs).append(ElasticNet_model_coefs))
B.index = ["Linear Regression Model","Lasso Regression Model","Ridge Regression Model","ElasticNet Regression Model"]

REPORT = B.reset_index().merge(A.reset_index())

REPORT = REPORT.set_index("index")
REPORT

sns.barplot(y  = REPORT.loc["ElasticNet Regression Model"][0:7].index,
           x = REPORT.loc["ElasticNet Regression Model"][0:7])

"""##**Insights :**

**Data Quality Assurance:** Ensure rigorous checks for data integrity, focusing on the absence of null values and identifying any outliers that may affect model accuracy.

**Variable Understanding:** Recognize the varied characteristics of variables like University Rating, SOP, and LOR strength, which exhibit features of both discrete random variables and ordinal numeric data. This understanding will be crucial for appropriate analysis and modeling.

**Feature Importance:** Prioritize features with strong correlations to the Chance of Admission, such as GRE score, TOEFL score, and CGPA, as they hold significant predictive power.

**Correlation Awareness:** Understand the positive correlations between SOP, LOR strength, University Rating, and the Chance of Admission, indicating their importance in influencing admission decisions.

**Research Impact:** Acknowledge the positive influence of research experience on admission chances, while also addressing any outliers within this category to ensure fair assessment.

**Normalization of Scores:** Normalize scores such as GRE, TOEFL, and CGPA to ensure consistency and comparability across different scales. This can enhance the interpretability of model coefficients and improve the accuracy of predictions.

**Exploratory Data Analysis (EDA):** Conduct comprehensive EDA to gain deeper insights into the distribution and relationships among variables. Visualizations such as histograms, boxplots, and scatter plots can reveal patterns and trends that inform modeling decisions.

**Feature Engineering:** Explore opportunities for feature engineering to create new variables or transform existing ones. For example, creating interaction terms between highly correlated predictors or deriving composite scores can capture additional information and enhance predictive power.

**Cross-Validation:** Implement cross-validation techniques such as k-fold cross-validation to assess model generalizability and stability. By partitioning the data into multiple subsets and iteratively training and testing the model, cross-validation provides a more robust estimate of model performance.

**Regularization Techniques:** Experiment with regularization techniques like Lasso and Ridge regression to address potential overfitting and improve model generalization. These techniques penalize the magnitude of coefficients, helping to prevent model complexity and enhance interpretability.

##**Recommendations:**

**Data Cleansing:** Conduct rigorous data cleansing procedures to eliminate irrelevant columns and ensure data completeness. Pay particular attention to handling missing values and outliers to maintain dataset integrity.

**Variable Treatment:** Develop appropriate strategies for handling variables with mixed characteristics, considering their unique roles in the admission process. Utilize statistical techniques such as VIF analysis to assess multicollinearity and ensure robust model performance.

**Modeling Strategy:** Incorporate variables with strong predictive power into modeling efforts, emphasizing those with high correlations with the Chance of Admission. Utilize statistical tests and diagnostic plots to assess model assumptions and refine model specifications.

**Continuous Monitoring:** Implement regular monitoring processes to track model performance and data quality. Continuously evaluate model accuracy and recalibrate as necessary to ensure reliable predictions and actionable insights in admissions decision-making.

**Continuous Learning:** Stay updated on advancements in predictive modeling techniques and admissions criteria. Engage in continuous learning through literature review, attending conferences, and collaborating with domain experts to refine modeling approaches and adapt to evolving trends.

**Collaborative Decision-Making:** Foster collaboration between data scientists, admissions officers, and domain experts to ensure alignment between model predictions and institutional objectives. Incorporate qualitative insights from admissions officers to complement quantitative modeling results and enhance decision-making.

**Ethical Considerations:** Prioritize ethical considerations in modeling practices, particularly in sensitive areas such as admissions. Implement fairness-aware modeling techniques to mitigate biases and ensure equitable treatment of all applicants, promoting transparency and trust in the admissions process.

**Documentation and Reporting:** Maintain detailed documentation of modeling processes, including data preprocessing steps, model selection criteria, and evaluation metrics. Transparent reporting facilitates reproducibility, fosters accountability, and enables stakeholders to make informed decisions based on model outputs.

https://colab.research.google.com/drive/1_1Ue2OwPVOb-8IuH6iVScm9pTNMJ39HD?usp=sharing
"""